{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从0实现多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "input_dim = 784\n",
    "hidden_dim = 256\n",
    "output_dim = 10\n",
    "\n",
    "w1 = torch.tensor(np.random.normal(0, 0.01, (input_dim, hidden_dim)), dtype = torch.float).requires_grad_(requires_grad=True)\n",
    "b1 = torch.zeros(hidden_dim, dtype = torch.float).requires_grad_(requires_grad=True)\n",
    "w2 = torch.tensor(np.random.normal(0, 0.01, (hidden_dim, output_dim)), dtype = torch.float).requires_grad_(requires_grad=True)\n",
    "b2 = torch.zeros(output_dim, dtype = torch.float).requires_grad_(requires_grad=True)\n",
    "\n",
    "params = [w1,b1,w2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义激活函数\n",
    "\n",
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    X_exp = X.exp()\n",
    "    return 1 / (1 + X_exp)\n",
    "\n",
    "def tanh(X):\n",
    "    X_exp = torch.exp(-2 * X)\n",
    "    return (1 - X_exp) / (1 + X_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def MLP(X):\n",
    "    X = X.view(-1, input_dim)\n",
    "    H = relu(torch.matmul(X, w1) + b1)\n",
    "    O = torch.matmul(H, w2) + b2\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "# crossentropy 自己的有问题\n",
    "def crossEntropy(y_hat, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))\n",
    "myloss = crossEntropy\n",
    "\n",
    "# 注意，pytorch的交叉熵默认是平均的\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.708, test acc 0.755\n",
      "epoch 2, loss 0.0019, train acc 0.824, test acc 0.820\n",
      "epoch 3, loss 0.0016, train acc 0.846, test acc 0.784\n",
      "epoch 4, loss 0.0016, train acc 0.855, test acc 0.832\n",
      "epoch 5, loss 0.0015, train acc 0.863, test acc 0.831\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "num_epochs, lr = 5, 100.0\n",
    "\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "\n",
    "\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train(MLP, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.713, test acc 0.797\n",
      "epoch 2, loss 0.0019, train acc 0.823, test acc 0.828\n",
      "epoch 3, loss 0.0017, train acc 0.843, test acc 0.829\n",
      "epoch 4, loss 0.0015, train acc 0.856, test acc 0.841\n",
      "epoch 5, loss 0.0015, train acc 0.862, test acc 0.850\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 100.0\n",
    "d2l.train_ch3(MLP, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用pytorch的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个falttern层\n",
    "class FlatternLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlatternLayer,self).__init__()\n",
    "    def forward(self,X):\n",
    "        return X.view(X.shape[0], -1)\n",
    "    \n",
    "model = torch.nn.Sequential(\n",
    "    FlatternLayer(),\n",
    "    torch.nn.Linear(input_dim, hidden_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim, output_dim)\n",
    ")\n",
    "\n",
    "for params in model.parameters():\n",
    "    torch.nn.init.normal_(params, mean=0, std=0.01)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.700, test acc 0.776\n",
      "epoch 2, loss 0.0019, train acc 0.819, test acc 0.817\n",
      "epoch 3, loss 0.0017, train acc 0.842, test acc 0.853\n",
      "epoch 4, loss 0.0015, train acc 0.857, test acc 0.828\n",
      "epoch 5, loss 0.0014, train acc 0.862, test acc 0.849\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 5\n",
    "train(model, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
