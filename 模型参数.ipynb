{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 模型参数的访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(4,3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3,1)\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2,4)\n",
    "Y = net(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Sequential实例中的模型参数层，可以通过parameters()和named_parameters()访问（返回迭代器），后者还会返回名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "print(type(net.parameters()))\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())\n",
    "for param in net.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "# 访问net中单层的参数\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size(), type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1 torch.Size([20, 20])\n"
     ]
    }
   ],
   "source": [
    "# Parameter是tensor子类，同时会被自动添加进模型的参数列表\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModule, self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20,20))\n",
    "        self.weight2 = torch.rand(20,20)\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "n = MyModule()\n",
    "for name, param in n.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1128, -0.0749, -0.2854, -0.0079],\n",
      "        [-0.0461, -0.2120,  0.4907, -0.0095],\n",
      "        [-0.0410, -0.4588,  0.3709, -0.3550]])\n",
      "None\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1603, 0.0415, 0.1438, 0.1853],\n",
      "        [0.6148, 0.3353, 0.5958, 0.6931]])\n"
     ]
    }
   ],
   "source": [
    "# 跟tensor一样，访问参数数值，访问梯度\n",
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad)\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型参数的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "0.weight tensor([[ 0.1128, -0.0749, -0.2854, -0.0079],\n",
      "        [-0.0461, -0.2120,  0.4907, -0.0095],\n",
      "        [-0.0410, -0.4588,  0.3709, -0.3550]])\n",
      "after\n",
      "0.weight tensor([[-1.6624e-02, -1.5824e-02, -6.0067e-03,  5.9786e-05],\n",
      "        [-9.5843e-03,  5.5858e-03, -2.0283e-02,  9.1880e-03],\n",
      "        [-1.0010e-02, -6.1504e-03, -6.5615e-04,  2.5002e-03]])\n",
      "before\n",
      "2.weight tensor([[0.0259, 0.1901, 0.5680]])\n",
      "after\n",
      "2.weight tensor([[ 0.0150, -0.0055,  0.0026]])\n"
     ]
    }
   ],
   "source": [
    "# N(0,0.01)初始化\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        print(\"before\")\n",
    "        print(name, param.data)\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(\"after\")\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "0.bias tensor([0.0000, 0.0000, 7.3857])\n",
      "after\n",
      "0.bias tensor([1., 1., 1.])\n",
      "before\n",
      "2.bias tensor([-0.])\n",
      "after\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# 常数初始化\n",
    "for name, param in net.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        print(\"before\")\n",
    "        print(name, param.data)\n",
    "        init.constant_(param, val=1)\n",
    "        print(\"after\")\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义初始化方法\n",
    "我们可以自定义初始化的方法，参考一下torch.nn.init.normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_(tensor, mean=0, std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现其实就是一个原地改变tensor的值且不记录梯度的函数（parameters本身就是tensor子类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "0.bias tensor([1., 1., 1.])\n",
      "after\n",
      "0.bias tensor([ 0.0000,  0.0000, -8.5646])\n",
      "before\n",
      "2.bias tensor([1.])\n",
      "after\n",
      "2.bias tensor([6.8773])\n"
     ]
    }
   ],
   "source": [
    "# 自定义\n",
    "# 一半概率初始化为0， 一半概率初始化为[-10,-5][5,10]的随机数\n",
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10,10)\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        print('before')\n",
    "        print(name, param.data)\n",
    "        init_weight(param)\n",
    "        print('after')\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型参数的共享\n",
    "两种方式    \n",
    "1. Module类forward函数多次调用一层\n",
    "2. 传入Sequential同一实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1,1, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    init.constant_(param, val=3)\n",
    "    print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0]) == id(net[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "# 反向传播时，共享的参数的梯度会累加\n",
    "x = torch.ones(1,1)\n",
    "y = net(x).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(net[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
